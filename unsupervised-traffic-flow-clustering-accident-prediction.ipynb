{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbe4a79-fffd-4c17-bf7c-477e93a0ade9",
   "metadata": {},
   "source": [
    "## Dataset Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba824a-604f-4a65-a63b-55844fee9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Checking if filtered_data.csv already exists\n",
    "if not os.path.exists('filtered_data.csv'):\n",
    "    df_huge = pd.read_csv('Accident_Information.csv', low_memory=False)\n",
    "\n",
    "    # Filter for Year 2014-2017\n",
    "    df = df_huge[df_huge['Year'].between(2014, 2017)].copy()\n",
    "\n",
    "    # Save to a new CSV\n",
    "    df.to_csv('filtered_data.csv', index=False)\n",
    "else:\n",
    "    print(\"filtered_data.csv already exists, skipping file creation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065de40e",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d8e66-3c5f-48b7-a48d-9a7fb8fedce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#above, we import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39705318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv('filtered_data.csv', low_memory=False)\n",
    "\n",
    "# I am using a different filtered dataset with about 550k rows/entries, as the original\n",
    "# had 2,000,000+ rows and was inefficient and consuming way too much memory and time\n",
    "\n",
    "# here, we display first few rows to get a preview of our data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a3fa5",
   "metadata": {},
   "source": [
    "## 2. Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give us a brief summary of the dataset\n",
    "df.info()\n",
    "print(\"\")\n",
    "print(\"______________________________________________________________\")\n",
    "print(\"\")\n",
    "# This gives us the number of null values \n",
    "print(df.isnull().sum())\n",
    "print(\"\")\n",
    "print(\"______________________________________________________________\")\n",
    "print(\"\")\n",
    "# This will tell us all the columns we have in our dataset\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c805b9",
   "metadata": {},
   "source": [
    "## 3. Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a79d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode 'Day_of_Week' and 'Road_Type', then convert to int32 for consistency\n",
    "\n",
    "le_day = LabelEncoder()\n",
    "le_road = LabelEncoder()\n",
    "\n",
    "df['Day_of_Week'] = le_day.fit_transform(df['Day_of_Week']).astype(np.int32)\n",
    "df['Road_Type'] = le_road.fit_transform(df['Road_Type']).astype(np.int32)\n",
    "\n",
    "\n",
    "# then we select features for clustering\n",
    "features = ['Day_of_Week', 'Number_of_Casualties']\n",
    "\n",
    "# create a variable X with the selected features\n",
    "X = df[features].copy()  # .copy() for safety cuz it creates a copy\n",
    "\n",
    "# check for missing values in selected features\n",
    "print(X.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "# after, we scale the features\n",
    "\n",
    "X = df[features].fillna(0)\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "df['Cluster'] = KMeans(n_clusters=3, random_state=42).fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f83daa-1acd-4d99-afc6-8a3e72b58355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check on the prepped data — shape, stats, and nulls\n",
    "\n",
    "print(X.shape)\n",
    "print(X.describe())\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a04ab7",
   "metadata": {},
   "source": [
    "## 4. Apply K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8fed7-a078-4604-a96a-4d1db86a1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we attempt the elbow method for optimal k, where inertia is used to see how\n",
    "# cluster compactness changes as k increases\n",
    "\n",
    "inertia = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "plt.plot(range(2, 10), inertia, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "#the \"Elbow\" is at 3 clusters, as we see the rate of decrease slow down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880aeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we fit K-Means with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# add cluster labels to DataFrame\n",
    "df['Cluster'] = kmeans.labels_\n",
    "\n",
    "# see how many rows ended up in each cluster\n",
    "df['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f69b9",
   "metadata": {},
   "source": [
    "## 5. Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500927c5-4154-4e2a-b277-6157e481f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we visualize the clusters with a barplot, where we can show the correlation between casualties\n",
    "# and day of the week between the clusters\n",
    "\n",
    "sns.barplot(\n",
    "    x=features[0],\n",
    "    y=features[1],\n",
    "    hue='Cluster',\n",
    "    data=df,\n",
    "    palette='viridis',\n",
    "    errorbar=None\n",
    ")\n",
    "\n",
    "plt.title(f'Average {features[1]} by {features[0]} and Cluster')\n",
    "plt.xlabel(features[0])\n",
    "plt.ylabel(f'Average {features[1]}')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09739d01-d4cd-4ff3-b147-2bccc359fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby(['Day_of_Week', 'Cluster']).size())\n",
    "\n",
    "# The clustering algorithm naturally separated the data into \n",
    "# patterns, where Cluster 2 dominates the early part of the week, \n",
    "# Cluster 0 dominates the second half, and Cluster 1 appears consistently \n",
    "# but scattered across all days.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2388415-f2b5-40c3-8ad0-f06854602b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce features to 2 principal components for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['Cluster'] = df['Cluster']\n",
    "\n",
    "# Sample for faster plotting\n",
    "pca_df_sample = pca_df\n",
    "\n",
    "\n",
    "# Scatter plot of clusters in PCA-reduced space\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df_sample, palette='viridis')\n",
    "plt.title('Clusters Visualized Using PCA Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster', loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd35f5-1df3-4cdb-b6fb-4cfd4a143bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis', s=10, alpha=0.5, label='Data')\n",
    "\n",
    "# Plot centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "centroids_2d = pca.transform(centroids)  # reduce centroids to 2D as well\n",
    "plt.scatter(centroids_2d[:, 0], centroids_2d[:, 1], c='red', s=100, marker='X', label='Centroids')\n",
    "\n",
    "plt.title(\"K-Means Clusters with Centroids (PCA 2D)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
